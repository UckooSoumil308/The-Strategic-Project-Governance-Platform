{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ§  Agent Control Plane â€” GPU Judge (Llama 3.2:1b)\n",
                "\n",
                "This notebook runs a local LLM-as-a-Judge on a Colab T4 GPU.  \n",
                "It exposes a **FastAPI** endpoint via **Ngrok** that our MERN backend calls to evaluate task strategic alignment.\n",
                "\n",
                "**Run Cell 1 first (setup), then Cell 2 (server + tunnel).** Copy the Ngrok URL printed at the bottom."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 1 â€” Install Dependencies & Pull Model\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\n",
                "# Install zstd (required by Ollama installer)\n",
                "!apt-get update -qq && apt-get install -y -qq zstd > /dev/null 2>&1\n",
                "print('âœ… zstd installed')\n",
                "\n",
                "# Install Ollama\n",
                "!curl -fsSL https://ollama.com/install.sh | sh\n",
                "print('âœ… Ollama installed')\n",
                "\n",
                "# Install Python packages (pin uvicorn <0.32 to avoid loop_factory bug)\n",
                "!pip install -q pyngrok fastapi 'uvicorn<0.32' nest-asyncio\n",
                "print('âœ… Python packages installed')\n",
                "\n",
                "# Start Ollama server in background\n",
                "import subprocess, time, os\n",
                "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
                "subprocess.Popen(\n",
                "    ['ollama', 'serve'],\n",
                "    stdout=subprocess.DEVNULL,\n",
                "    stderr=subprocess.DEVNULL\n",
                ")\n",
                "time.sleep(5)  # Wait for server to initialize\n",
                "print('âœ… Ollama server started')\n",
                "\n",
                "# Pull the lightweight Llama 3.2:1b model (optimized for T4 GPU)\n",
                "!ollama pull llama3.2:1b\n",
                "\n",
                "print('\\nâœ… Setup complete â€” Ollama running with llama3.2:1b loaded.')\n",
                "print('   Now run Cell 2 to start the Judge server + Ngrok tunnel.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mFailed to restart the Kernel. \n",
                        "\u001b[1;31mInvalid response: 404 Not Found. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 2 â€” FastAPI Judge Server + Ngrok Tunnel\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#\n",
                "# âš ï¸  PASTE YOUR NGROK AUTH TOKEN BELOW BEFORE RUNNING THIS CELL\n",
                "#\n",
                "\n",
                "NGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN\"  # <-- Replace this!\n",
                "\n",
                "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "\n",
                "import json, re, subprocess, threading, time\n",
                "from fastapi import FastAPI, HTTPException\n",
                "from fastapi.middleware.cors import CORSMiddleware\n",
                "from pydantic import BaseModel\n",
                "\n",
                "app = FastAPI(title='TaskEase Governance Judge')\n",
                "\n",
                "app.add_middleware(\n",
                "    CORSMiddleware,\n",
                "    allow_origins=['*'],\n",
                "    allow_methods=['*'],\n",
                "    allow_headers=['*'],\n",
                ")\n",
                "\n",
                "# â”€â”€ Request / Response schemas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "class EvaluateRequest(BaseModel):\n",
                "    taskDetails: str\n",
                "    projectOKRs: str\n",
                "\n",
                "class EvaluateResponse(BaseModel):\n",
                "    approved: bool\n",
                "    reason: str\n",
                "\n",
                "# â”€â”€ LLM helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "def query_llama(task_details: str, project_okrs: str) -> dict:\n",
                "    \"\"\"Send a structured prompt to llama3.2:1b via Ollama CLI and parse JSON.\"\"\"\n",
                "\n",
                "    prompt = (\n",
                "        'You are a Strategic Governance Judge for a project management platform. '\n",
                "        'Your job is to evaluate whether a proposed task is strategically aligned '\n",
                "        \"with the project's OKRs.\\n\\n\"\n",
                "        f'PROJECT OKRs:\\n{project_okrs}\\n\\n'\n",
                "        f'PROPOSED TASK:\\n{task_details}\\n\\n'\n",
                "        'INSTRUCTIONS:\\n'\n",
                "        '1. Evaluate whether this task contributes to or supports the stated OKRs.\\n'\n",
                "        '2. If the task is clearly unrelated, addresses a different goal, or would '\n",
                "        'cause strategic drift, set approved to false.\\n'\n",
                "        '3. If the task supports, enables, or directly contributes to any OKR, '\n",
                "        'set approved to true.\\n'\n",
                "        '4. Provide a concise reason (1-2 sentences) explaining your decision.\\n\\n'\n",
                "        'You MUST respond with ONLY valid JSON in this exact format, no other text:\\n'\n",
                "        '{\"approved\": true, \"reason\": \"your reason here\"}\\n'\n",
                "        'or\\n'\n",
                "        '{\"approved\": false, \"reason\": \"your reason here\"}'\n",
                "    )\n",
                "\n",
                "    try:\n",
                "        result = subprocess.run(\n",
                "            ['ollama', 'run', 'llama3.2:1b', prompt],\n",
                "            capture_output=True,\n",
                "            text=True,\n",
                "            timeout=60\n",
                "        )\n",
                "        raw = result.stdout.strip()\n",
                "        print(f'[Judge Raw Output] {raw}')\n",
                "\n",
                "        # Extract JSON from response (handles markdown fences or extra text)\n",
                "        json_match = re.search(r'\\{[^{}]*\"approved\"[^{}]*\\}', raw)\n",
                "        if json_match:\n",
                "            parsed = json.loads(json_match.group())\n",
                "            return {\n",
                "                'approved': bool(parsed.get('approved', True)),\n",
                "                'reason': str(parsed.get('reason', 'No reason provided.'))\n",
                "            }\n",
                "\n",
                "        # Fallback: approve if we can't parse (fail-open)\n",
                "        return {'approved': True, 'reason': f'Judge response could not be parsed. Raw: {raw[:200]}'}\n",
                "\n",
                "    except subprocess.TimeoutExpired:\n",
                "        return {'approved': True, 'reason': 'Judge timed out â€” defaulting to approved.'}\n",
                "    except Exception as e:\n",
                "        return {'approved': True, 'reason': f'Judge error: {str(e)} â€” defaulting to approved.'}\n",
                "\n",
                "# â”€â”€ Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "@app.post('/evaluate-task', response_model=EvaluateResponse)\n",
                "async def evaluate_task(req: EvaluateRequest):\n",
                "    \"\"\"Evaluate a proposed task against project OKRs using local LLM.\"\"\"\n",
                "    if not req.taskDetails or not req.projectOKRs:\n",
                "        raise HTTPException(status_code=400, detail='taskDetails and projectOKRs are required.')\n",
                "\n",
                "    result = query_llama(req.taskDetails, req.projectOKRs)\n",
                "    print(f'[Judge Decision] approved={result[\"approved\"]} | reason={result[\"reason\"]}')\n",
                "    return EvaluateResponse(**result)\n",
                "\n",
                "@app.get('/health')\n",
                "async def health():\n",
                "    return {'status': 'ok', 'model': 'llama3.2:1b'}\n",
                "\n",
                "# â”€â”€ Start FastAPI in background thread â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "import uvicorn\n",
                "\n",
                "def start_server():\n",
                "    \"\"\"Run uvicorn in a way compatible with nest_asyncio.\"\"\"\n",
                "    config = uvicorn.Config(app, host='0.0.0.0', port=8000, log_level='info')\n",
                "    server = uvicorn.Server(config)\n",
                "    import asyncio\n",
                "    loop = asyncio.new_event_loop()\n",
                "    loop.run_until_complete(server.serve())\n",
                "\n",
                "server_thread = threading.Thread(target=start_server, daemon=True)\n",
                "server_thread.start()\n",
                "time.sleep(3)  # Let server start\n",
                "print('ğŸš€ FastAPI Judge Server running on port 8000')\n",
                "\n",
                "# â”€â”€ Ngrok tunnel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "from pyngrok import ngrok\n",
                "\n",
                "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
                "public_url = ngrok.connect(8000)\n",
                "\n",
                "print('\\n' + '=' * 60)\n",
                "print('ğŸŒ NGROK TUNNEL ACTIVE')\n",
                "print(f'   Public URL: {public_url}')\n",
                "print(f'   Test:  GET  {public_url}/health')\n",
                "print(f'   Judge: POST {public_url}/evaluate-task')\n",
                "print('=' * 60)\n",
                "print('\\nğŸ“‹ Copy the URL above and paste it back to the AI assistant.')\n",
                "print('   It will be set as COLAB_JUDGE_URL in your .env file.\\n')\n",
                "\n",
                "# Keep cell alive so the server and tunnel stay running\n",
                "try:\n",
                "    while True:\n",
                "        time.sleep(1)  \n",
                "except KeyboardInterrupt:\n",
                "    print('\\nShutting down...')\n",
                "    ngrok.kill()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
