{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  Agent Control Plane â€” Windows Local Judge (Llama 3.2:1b)\n",
    "\n",
    "This notebook runs a local LLM-as-a-Judge natively on your Windows PC.  \n",
    "It exposes a **FastAPI** endpoint via **Ngrok** that our MERN backend calls to evaluate task strategic alignment.\n",
    "\n",
    "**Run Cell 1 first (setup), then Cell 2 (server + tunnel).** Copy the Ngrok URL printed at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 1 â€” Install Dependencies & Pull Model (WINDOWS NATIVE)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print('âš ï¸ Make sure you have installed the Windows version of Ollama from ollama.com!')\n",
    "\n",
    "# Install Python packages\n",
    "%pip install pyngrok fastapi \"uvicorn<0.32\" nest-asyncio\n",
    "print('âœ… Python packages installed')\n",
    "\n",
    "# Start Ollama server in background (using shell=True for Windows native)\n",
    "import subprocess, time, os\n",
    "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "\n",
    "try:\n",
    "    subprocess.Popen(\n",
    "        ['ollama', 'serve'],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL,\n",
    "        shell=True\n",
    "    )\n",
    "    time.sleep(5)  # Wait for server to initialize\n",
    "    print('âœ… Local Windows Ollama server started')\n",
    "except Exception as e:\n",
    "    print(f'âš ï¸ Warning: {e}\\n(If Ollama is already running in your system tray, you can ignore this.)')\n",
    "\n",
    "# Pull the lightweight Llama 3.2:1b model\n",
    "print('Pulling model (this may take a moment)...')\n",
    "!ollama pull llama3.2:1b\n",
    "\n",
    "print('\\nâœ… Setup complete â€” Ollama running with llama3.2:1b loaded.')\n",
    "print('   Now run Cell 2 to start the Judge server + Ngrok tunnel.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 2 â€” FastAPI Judge Server + Ngrok Tunnel\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#\n",
    "# âš ï¸  PASTE YOUR NGROK AUTH TOKEN BELOW BEFORE RUNNING THIS CELL\n",
    "#\n",
    "\n",
    "NGROK_AUTH_TOKEN = \"\"  # <-- Replace with your actual token: 39toLcgq9S972ZRxYhcOMBRuugp_3FfEdk2NVjYudH9nDizuT\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import json, re, subprocess, threading, time\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI(title='TaskEase Governance Judge')\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=['*'],\n",
    "    allow_methods=['*'],\n",
    "    allow_headers=['*'],\n",
    ")\n",
    "\n",
    "# â”€â”€ Request / Response schemas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class EvaluateRequest(BaseModel):\n",
    "    taskDetails: str\n",
    "    projectOKRs: str\n",
    "\n",
    "class EvaluateResponse(BaseModel):\n",
    "    approved: bool\n",
    "    reason: str\n",
    "\n",
    "# â”€â”€ LLM helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def query_llama(task_details: str, project_okrs: str) -> dict:\n",
    "    \"\"\"Send a structured prompt to llama3.2:1b via Ollama CLI and parse JSON.\"\"\"\n",
    "\n",
    "    prompt = (\n",
    "        'You are a Strategic Governance Judge for a project management platform. '\n",
    "        'Your job is to evaluate whether a proposed task is strategically aligned '\n",
    "        \"with the project's OKRs.\\n\\n\"\n",
    "        f'PROJECT OKRs:\\n{project_okrs}\\n\\n'\n",
    "        f'PROPOSED TASK:\\n{task_details}\\n\\n'\n",
    "        'INSTRUCTIONS:\\n'\n",
    "        '1. Evaluate whether this task contributes to or supports the stated OKRs.\\n'\n",
    "        '2. If the task is clearly unrelated, addresses a different goal, or would '\n",
    "        '3. Only if the task is egregiously unrelated or explicitly forbidden, set approved to false.\\\\n'\\n\n",
    "        '3. If the task supports, enables, or directly contributes to any OKR, '\n",
    "\n",
    "        '4. Provide a concise reason (1-2 sentences) explaining your decision.\\n\\n'\n",
    "        'You MUST respond with ONLY valid JSON in this exact format, no other text:\\n'\n",
    "        '{\"approved\": true, \"reason\": \"your reason here\"}\\n'\n",
    "        'or\\n'\n",
    "        '{\"approved\": false, \"reason\": \"your reason here\"}'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['ollama', 'run', 'llama3.2:1b', prompt],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=60,\n",
    "            shell=True # Crucial for Windows so it finds 'ollama' executable in PATH\n",
    "        )\n",
    "        raw = result.stdout.strip()\n",
    "        print(f'[Judge Raw Output] {raw}')\n",
    "\n",
    "        # Extract JSON from response (handles markdown fences or extra text)\n",
    "        json_match = re.search(r'\\{[^{}]*\"approved\"[^{}]*\\}', raw)\n",
    "        if json_match:\n",
    "            parsed = json.loads(json_match.group())\n",
    "            return {\n",
    "                'approved': bool(parsed.get('approved', True)),\n",
    "                'reason': str(parsed.get('reason', 'No reason provided.'))\n",
    "            }\n",
    "\n",
    "        # Fallback: approve if we can't parse (fail-open)\n",
    "        return {'approved': True, 'reason': f'Judge response could not be parsed. Raw: {raw[:200]}'}\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return {'approved': True, 'reason': 'Judge timed out â€” defaulting to approved.'}\n",
    "    except Exception as e:\n",
    "        return {'approved': True, 'reason': f'Judge error: {str(e)} â€” defaulting to approved.'}\n",
    "\n",
    "# â”€â”€ Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@app.post('/evaluate-task', response_model=EvaluateResponse)\n",
    "async def evaluate_task(req: EvaluateRequest):\n",
    "    \"\"\"Evaluate a proposed task against project OKRs using local LLM.\"\"\"\n",
    "    if not req.taskDetails or not req.projectOKRs:\n",
    "        raise HTTPException(status_code=400, detail='taskDetails and projectOKRs are required.')\n",
    "\n",
    "    result = query_llama(req.taskDetails, req.projectOKRs)\n",
    "    print(f'[Judge Decision] approved={result[\"approved\"]} | reason={result[\"reason\"]}')\n",
    "    return EvaluateResponse(**result)\n",
    "\n",
    "@app.get('/health')\n",
    "async def health():\n",
    "    return {'status': 'ok', 'model': 'llama3.2:1b', 'mode': 'Windows Local'}\n",
    "\n",
    "\n",
    "@app.post('/api/ai/refine-schedule', response_model=ProjectSchedule)\n",
    "async def refine_schedule(req: RefineRequest):\n",
    "    \"\"\"Refine an existing project schedule based on user instructions using local LLM.\"\"\"\n",
    "    system_prompt = (\n",
    "        'You are an expert Project Manager. Break the following project description \\\\n'\n",
    "        'down into a logical, sequential Work Breakdown Structure (WBS).  \\\\n'\n",
    "        'Use logical_predecessors to map dependencies to the indices of previous tasks.\\n\\n'\n",
    "        f'Current Schedule:\\n{json.dumps(req.current_schedule, indent=2)}\\n\\n'\n",
    "        f'User Instructions:\\n{req.user_instructions}\\n\\n'\n",
    "        'Refine the schedule above based on the instructions. Return the COMPLETE updated schedule.'\n",
    "    )\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3.2:1b',\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_prompt}\n",
    "            ],\n",
    "            format=ProjectSchedule.model_json_schema()\n",
    "        )\n",
    "        parsed_data = json.loads(response['message']['content'])\n",
    "        return parsed_data\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# â”€â”€ Start FastAPI in background thread â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def start_server():\n",
    "    \"\"\"Run uvicorn in a way compatible with nest_asyncio.\"\"\"\n",
    "    config = uvicorn.Config(app, host='0.0.0.0', port=8000, log_level='info')\n",
    "    server = uvicorn.Server(config)\n",
    "    import asyncio\n",
    "    loop = asyncio.new_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "server_thread = threading.Thread(target=start_server, daemon=True)\n",
    "server_thread.start()\n",
    "time.sleep(3)  # Let server start\n",
    "print('ğŸš€ FastAPI Windows Local Judge Server running on port 8000')\n",
    "\n",
    "# â”€â”€ Ngrok tunnel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pyngrok import ngrok\n",
    "\n",
    "try:\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "    public_url = ngrok.connect(8000)\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('ğŸŒ NGROK TUNNEL ACTIVE')\n",
    "    print(f'   Public URL: {public_url}')\n",
    "    print(f'   Test:  GET  {public_url}/health')\n",
    "    print(f'   Judge: POST {public_url}/evaluate-task')\n",
    "    print('=' * 60)\n",
    "    print('\\nğŸ“‹ Copy the URL above and paste it back to the AI assistant.')\n",
    "    print('   It will be set as COLAB_JUDGE_URL in your .env file.\\n')\n",
    "except Exception as e:\n",
    "    print(f'âš ï¸ Error starting Ngrok (did you paste your auth token?): {e}')\n",
    "\n",
    "# Keep cell alive so the server and tunnel stay running\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)  \n",
    "except KeyboardInterrupt:\n",
    "    print('\\nShutting down...')\n",
    "    ngrok.kill()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
